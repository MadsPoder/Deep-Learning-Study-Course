{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madspoderpetersen/deep-traffic/venv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import cv2\n",
    "import keras.preprocessing.image\n",
    "import keras_retinanet\n",
    "import math\n",
    "from keras_retinanet import models, losses\n",
    "from keras_retinanet.models.retinanet import retinanet_bbox\n",
    "from keras_retinanet.models.resnet import resnet50_retinanet\n",
    "from keras_retinanet.preprocessing.csv_generator import CSVGenerator\n",
    "#from generator import CustomCSVGenerator\n",
    "from keras_retinanet.callbacks import RedirectModel\n",
    "from keras_retinanet.utils.transform import random_transform_generator\n",
    "#from keras_retinanet.callbacks.eval import Evaluate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from shared import get_session, TrainingType\n",
    "from callbacks import save_weights_vis, Evaluate\n",
    "from paths import *\n",
    "\n",
    "#Create folders\n",
    "if not os.path.exists(SNAPSHOTS_DIR):\n",
    "    os.makedirs(SNAPSHOTS_DIR)\n",
    "    \n",
    "if not os.path.exists(LOGS_DIR):\n",
    "    os.makedirs(LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(num_classes, weights, freeze_backbone=None):\n",
    "    modifier = freeze_model if freeze_backbone else None\n",
    "    model = models.resnet.resnet_retinanet(num_classes, modifier=modifier)\n",
    "    if weights == \"imagenet\":\n",
    "        backbone = models.backbone('resnet50')\n",
    "        imgnet_weights = backbone.download_imagenet()\n",
    "        #Load by_name only inputs into the resnet backbone (Tak Thomas)\n",
    "        model.load_weights(imgnet_weights, skip_mismatch=True, by_name=True)\n",
    "    elif weights:\n",
    "        model.load_weights(weights, skip_mismatch=True)\n",
    "\n",
    "    prediction_model = retinanet_bbox(model=model)\n",
    "    \n",
    "    model.trainable = True\n",
    "\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True \n",
    "\n",
    "    model.compile(\n",
    "        loss={\n",
    "            'regression'    : losses.smooth_l1(),\n",
    "            'classification': losses.focal()\n",
    "        },\n",
    "        optimizer=keras.optimizers.adam(lr=1e-5, clipnorm=0.001),\n",
    "        \n",
    "    )\n",
    "    #metrics=['accuracy'] kan inkluderes i model.compile\n",
    "    \n",
    "    model.trainable = True\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True \n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    return model, prediction_model\n",
    "\n",
    "def create_train_generator(batch_size):\n",
    "    transform_generator = random_transform_generator(\n",
    "            min_rotation=-0.1,\n",
    "            max_rotation=0.1,\n",
    "            min_translation=(-0.1, -0.1),\n",
    "            max_translation=(0.1, 0.1),\n",
    "            min_shear=-0.1,\n",
    "            max_shear=0.1,\n",
    "            min_scaling=(0.9, 0.9),\n",
    "            max_scaling=(1.1, 1.1),\n",
    "            flip_x_chance=0.5,\n",
    "            #flip_y_chance=0.5,\n",
    "        )\n",
    "    \n",
    "    train_generator = CSVGenerator(\n",
    "        TRAIN_GROUND_TRUTH_COMBINED,\n",
    "        CLASS_MAPPINGS_COMBINED,\n",
    "        batch_size=batch_size,\n",
    "        transform_generator=transform_generator\n",
    "    )\n",
    "    return train_generator\n",
    "\n",
    "def create_test_generator(batch_size):\n",
    "    test_generator = CSVGenerator(\n",
    "        TEST_GROUND_TRUTH_COMBINED,\n",
    "        CLASS_MAPPINGS_COMBINED,\n",
    "        shuffle_groups=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return test_generator\n",
    "\n",
    "def create_callbacks(model, prediction_model, batch_size, test_generator, train_generator, resume):\n",
    "    callbacks = []\n",
    "\n",
    "    # save the prediction model\n",
    "    if resume:\n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(SNAPSHOTS_DIR, 'custom_resnet50_lolle_resume_imgnet_{epoch:02d}.h5'), verbose=1, period=5)\n",
    "    else:\n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(SNAPSHOTS_DIR, 'custom_resnet50_imgnet_{epoch:02d}.h5'), verbose=1, period=5)\n",
    "\n",
    "    #checkpoint = RedirectModel(checkpoint, model)\n",
    "    callbacks.append(checkpoint)\n",
    "    \n",
    "    #Visualisation of weights\n",
    "    vis_weights = None\n",
    "    if resume:\n",
    "        vis_weights = save_weights_vis(os.path.join(SNAPSHOTS_DIR, 'custom_resnet50_resume_imgnet_{}.png'))\n",
    "    else:\n",
    "        vis_weights = save_weights_vis(os.path.join(SNAPSHOTS_DIR, 'custom_resnet50_imgnet_{}.png'))\n",
    "    callbacks.append(vis_weights)\n",
    "    \n",
    "    #Reduce learning rate on plateu (tidligere loss, val_loss)\n",
    "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, verbose=1, mode='auto', cooldown=0, min_lr=0.001)\n",
    "    #callbacks.append(lr_scheduler)\n",
    "    \n",
    "    #Early stop\n",
    "    early_stop = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "    callbacks.append(early_stop)\n",
    "    \n",
    "    #For tensorboard\n",
    "    tensorboard_callb = keras.callbacks.TensorBoard(log_dir=LOGS_DIR, histogram_freq=0, batch_size=batch_size, \n",
    "                                                    write_graph=True, write_grads=False, write_images=True, \n",
    "                                                    embeddings_freq=0, embeddings_layer_names=None, \n",
    "                                                    embeddings_metadata=None)\n",
    "    callbacks.append(tensorboard_callb)\n",
    "    \n",
    "    #Evaluation for the test/train set\n",
    "    evaluation_test = Evaluate(test_generator, tensorboard=tensorboard_callb, verbose=0, train=False)\n",
    "    evaluation_test = RedirectModel(evaluation_test, prediction_model)\n",
    "    callbacks.append(evaluation_test)\n",
    "    evaluation_train = Evaluate(train_generator, tensorboard=tensorboard_callb, verbose=0, train=True)\n",
    "    evaluation_train = RedirectModel(evaluation_train, prediction_model)\n",
    "    callbacks.append(evaluation_train)\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/madspoderpetersen/deep-traffic/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Epoch 1/75\n",
      "151/151 [==============================] - 147s 975ms/step - loss: 5.0200 - regression_loss: 3.8527 - classification_loss: 1.1673\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_1.png\n",
      "Epoch 2/75\n",
      "151/151 [==============================] - 135s 895ms/step - loss: 4.2763 - regression_loss: 3.3739 - classification_loss: 0.9024\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_2.png\n",
      "Epoch 3/75\n",
      "151/151 [==============================] - 135s 895ms/step - loss: 3.7296 - regression_loss: 3.0850 - classification_loss: 0.6446\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_3.png\n",
      "Epoch 4/75\n",
      "151/151 [==============================] - 135s 894ms/step - loss: 3.2008 - regression_loss: 2.6163 - classification_loss: 0.5845\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_4.png\n",
      "Epoch 5/75\n",
      "151/151 [==============================] - 135s 896ms/step - loss: 2.7823 - regression_loss: 2.2330 - classification_loss: 0.5494\n",
      "\n",
      "Epoch 00005: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_05.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_5.png\n",
      "Epoch 6/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 2.5166 - regression_loss: 1.9960 - classification_loss: 0.5206\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_6.png\n",
      "Epoch 7/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 2.3235 - regression_loss: 1.8169 - classification_loss: 0.5066\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_7.png\n",
      "Epoch 8/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 2.2011 - regression_loss: 1.7146 - classification_loss: 0.4865\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_8.png\n",
      "Epoch 9/75\n",
      "151/151 [==============================] - 135s 894ms/step - loss: 2.0746 - regression_loss: 1.6062 - classification_loss: 0.4684\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_9.png\n",
      "Epoch 10/75\n",
      "151/151 [==============================] - 135s 894ms/step - loss: 1.9897 - regression_loss: 1.5365 - classification_loss: 0.4532\n",
      "\n",
      "Epoch 00010: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_10.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_10.png\n",
      "Epoch 11/75\n",
      "151/151 [==============================] - 135s 894ms/step - loss: 1.8783 - regression_loss: 1.4478 - classification_loss: 0.4305\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_11.png\n",
      "Epoch 12/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 1.8542 - regression_loss: 1.4402 - classification_loss: 0.4141\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_12.png\n",
      "Epoch 13/75\n",
      "151/151 [==============================] - 135s 897ms/step - loss: 1.7606 - regression_loss: 1.3775 - classification_loss: 0.3831\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_13.png\n",
      "Epoch 14/75\n",
      "151/151 [==============================] - 135s 894ms/step - loss: 1.7230 - regression_loss: 1.3466 - classification_loss: 0.3764\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_14.png\n",
      "Epoch 15/75\n",
      "151/151 [==============================] - 135s 896ms/step - loss: 1.6755 - regression_loss: 1.3062 - classification_loss: 0.3693\n",
      "\n",
      "Epoch 00015: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_15.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_15.png\n",
      "Epoch 16/75\n",
      "151/151 [==============================] - 135s 896ms/step - loss: 1.5634 - regression_loss: 1.2185 - classification_loss: 0.3449\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_16.png\n",
      "Epoch 17/75\n",
      "151/151 [==============================] - 135s 894ms/step - loss: 1.5394 - regression_loss: 1.2002 - classification_loss: 0.3392\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_17.png\n",
      "Epoch 18/75\n",
      "151/151 [==============================] - 135s 897ms/step - loss: 1.4758 - regression_loss: 1.1456 - classification_loss: 0.3302\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_18.png\n",
      "Epoch 19/75\n",
      "151/151 [==============================] - 135s 895ms/step - loss: 1.4429 - regression_loss: 1.1471 - classification_loss: 0.2958\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_19.png\n",
      "Epoch 20/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 1.4391 - regression_loss: 1.1563 - classification_loss: 0.2828\n",
      "\n",
      "Epoch 00020: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_20.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_20.png\n",
      "Epoch 21/75\n",
      "151/151 [==============================] - 135s 896ms/step - loss: 1.2996 - regression_loss: 1.0406 - classification_loss: 0.2590\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_21.png\n",
      "Epoch 22/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 1.3128 - regression_loss: 1.0555 - classification_loss: 0.2573\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_22.png\n",
      "Epoch 23/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 1.2426 - regression_loss: 1.0059 - classification_loss: 0.2367\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_23.png\n",
      "Epoch 24/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 1.2306 - regression_loss: 0.9975 - classification_loss: 0.2331\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_24.png\n",
      "Epoch 25/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 1.2243 - regression_loss: 0.9988 - classification_loss: 0.2255\n",
      "\n",
      "Epoch 00025: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_25.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_25.png\n",
      "Epoch 26/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 1.1835 - regression_loss: 0.9579 - classification_loss: 0.2256\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_26.png\n",
      "Epoch 27/75\n",
      "151/151 [==============================] - 135s 891ms/step - loss: 1.1616 - regression_loss: 0.9428 - classification_loss: 0.2189\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_27.png\n",
      "Epoch 28/75\n",
      "151/151 [==============================] - 134s 890ms/step - loss: 1.1401 - regression_loss: 0.9285 - classification_loss: 0.2116\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_28.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/75\n",
      "151/151 [==============================] - 135s 894ms/step - loss: 1.1396 - regression_loss: 0.9323 - classification_loss: 0.2073\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_29.png\n",
      "Epoch 30/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 1.1156 - regression_loss: 0.9070 - classification_loss: 0.2086\n",
      "\n",
      "Epoch 00030: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_30.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_30.png\n",
      "Epoch 31/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 1.0204 - regression_loss: 0.8262 - classification_loss: 0.1941\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_31.png\n",
      "Epoch 32/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 1.0206 - regression_loss: 0.8250 - classification_loss: 0.1956\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_32.png\n",
      "Epoch 33/75\n",
      "151/151 [==============================] - 135s 891ms/step - loss: 1.0144 - regression_loss: 0.8233 - classification_loss: 0.1911\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_33.png\n",
      "Epoch 34/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 1.0283 - regression_loss: 0.8337 - classification_loss: 0.1946\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_34.png\n",
      "Epoch 35/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 1.0275 - regression_loss: 0.8360 - classification_loss: 0.1915\n",
      "\n",
      "Epoch 00035: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_35.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_35.png\n",
      "Epoch 36/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 0.9670 - regression_loss: 0.7869 - classification_loss: 0.1801\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_36.png\n",
      "Epoch 37/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 0.9756 - regression_loss: 0.7995 - classification_loss: 0.1761\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_37.png\n",
      "Epoch 38/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 0.9160 - regression_loss: 0.7443 - classification_loss: 0.1717\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_38.png\n",
      "Epoch 39/75\n",
      "151/151 [==============================] - 135s 895ms/step - loss: 0.9457 - regression_loss: 0.7636 - classification_loss: 0.1821\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_39.png\n",
      "Epoch 40/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 0.9074 - regression_loss: 0.7421 - classification_loss: 0.1653\n",
      "\n",
      "Epoch 00040: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_40.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_40.png\n",
      "Epoch 41/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 0.8964 - regression_loss: 0.7302 - classification_loss: 0.1662\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_41.png\n",
      "Epoch 42/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 0.8625 - regression_loss: 0.7036 - classification_loss: 0.1589\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_42.png\n",
      "Epoch 43/75\n",
      "151/151 [==============================] - 135s 894ms/step - loss: 0.8653 - regression_loss: 0.7028 - classification_loss: 0.1625\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_43.png\n",
      "Epoch 44/75\n",
      "151/151 [==============================] - 135s 896ms/step - loss: 0.8472 - regression_loss: 0.6880 - classification_loss: 0.1592\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_44.png\n",
      "Epoch 45/75\n",
      "151/151 [==============================] - 135s 891ms/step - loss: 0.8307 - regression_loss: 0.6758 - classification_loss: 0.1548\n",
      "\n",
      "Epoch 00045: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_45.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_45.png\n",
      "Epoch 46/75\n",
      "151/151 [==============================] - 135s 895ms/step - loss: 0.8288 - regression_loss: 0.6755 - classification_loss: 0.1533\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_46.png\n",
      "Epoch 47/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 0.8375 - regression_loss: 0.6812 - classification_loss: 0.1563\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_47.png\n",
      "Epoch 48/75\n",
      "151/151 [==============================] - 135s 895ms/step - loss: 0.8106 - regression_loss: 0.6608 - classification_loss: 0.1498\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_48.png\n",
      "Epoch 49/75\n",
      "151/151 [==============================] - 135s 895ms/step - loss: 0.7968 - regression_loss: 0.6470 - classification_loss: 0.1498\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_49.png\n",
      "Epoch 50/75\n",
      "151/151 [==============================] - 135s 894ms/step - loss: 0.8276 - regression_loss: 0.6777 - classification_loss: 0.1499\n",
      "\n",
      "Epoch 00050: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_50.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_50.png\n",
      "Epoch 51/75\n",
      "151/151 [==============================] - 134s 891ms/step - loss: 0.7895 - regression_loss: 0.6474 - classification_loss: 0.1421\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_51.png\n",
      "Epoch 52/75\n",
      "151/151 [==============================] - 134s 890ms/step - loss: 0.7671 - regression_loss: 0.6239 - classification_loss: 0.1432\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_52.png\n",
      "Epoch 53/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 0.7479 - regression_loss: 0.6125 - classification_loss: 0.1355\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_53.png\n",
      "Epoch 54/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 0.7436 - regression_loss: 0.6062 - classification_loss: 0.1374\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_54.png\n",
      "Epoch 55/75\n",
      "151/151 [==============================] - 134s 890ms/step - loss: 0.7297 - regression_loss: 0.6001 - classification_loss: 0.1297\n",
      "\n",
      "Epoch 00055: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_55.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_55.png\n",
      "Epoch 56/75\n",
      "151/151 [==============================] - 134s 891ms/step - loss: 0.7720 - regression_loss: 0.6395 - classification_loss: 0.1325\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_56.png\n",
      "Epoch 57/75\n",
      "151/151 [==============================] - 134s 890ms/step - loss: 0.7528 - regression_loss: 0.6141 - classification_loss: 0.1387\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_57.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/75\n",
      "151/151 [==============================] - 134s 889ms/step - loss: 0.7158 - regression_loss: 0.5901 - classification_loss: 0.1256\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_58.png\n",
      "Epoch 59/75\n",
      "151/151 [==============================] - 134s 891ms/step - loss: 0.7287 - regression_loss: 0.6018 - classification_loss: 0.1269\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_59.png\n",
      "Epoch 60/75\n",
      "151/151 [==============================] - 134s 889ms/step - loss: 0.7308 - regression_loss: 0.6027 - classification_loss: 0.1281\n",
      "\n",
      "Epoch 00060: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_60.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_60.png\n",
      "Epoch 61/75\n",
      "151/151 [==============================] - 135s 891ms/step - loss: 0.7058 - regression_loss: 0.5806 - classification_loss: 0.1252\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_61.png\n",
      "Epoch 62/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 0.7006 - regression_loss: 0.5772 - classification_loss: 0.1234\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_62.png\n",
      "Epoch 63/75\n",
      "151/151 [==============================] - 135s 891ms/step - loss: 0.6825 - regression_loss: 0.5651 - classification_loss: 0.1175\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_63.png\n",
      "Epoch 64/75\n",
      "151/151 [==============================] - 135s 891ms/step - loss: 0.6949 - regression_loss: 0.5736 - classification_loss: 0.1213\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_64.png\n",
      "Epoch 65/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 0.6761 - regression_loss: 0.5545 - classification_loss: 0.1216\n",
      "\n",
      "Epoch 00065: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_65.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_65.png\n",
      "Epoch 66/75\n",
      "151/151 [==============================] - 135s 891ms/step - loss: 0.6428 - regression_loss: 0.5266 - classification_loss: 0.1163\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_66.png\n",
      "Epoch 67/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 0.6693 - regression_loss: 0.5528 - classification_loss: 0.1165\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_67.png\n",
      "Epoch 68/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 0.6310 - regression_loss: 0.5204 - classification_loss: 0.1106\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_68.png\n",
      "Epoch 69/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 0.6694 - regression_loss: 0.5502 - classification_loss: 0.1192\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_69.png\n",
      "Epoch 70/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 0.6309 - regression_loss: 0.5211 - classification_loss: 0.1098\n",
      "\n",
      "Epoch 00070: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_70.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_70.png\n",
      "Epoch 71/75\n",
      "151/151 [==============================] - 134s 890ms/step - loss: 0.6472 - regression_loss: 0.5377 - classification_loss: 0.1095\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_71.png\n",
      "Epoch 72/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 0.6516 - regression_loss: 0.5400 - classification_loss: 0.1116\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_72.png\n",
      "308/310\r"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "training_type = TrainingType.IMGNET\n",
    "batch_size = 4\n",
    "\n",
    "keras.backend.tensorflow_backend.set_session(get_session())\n",
    "train_generator = create_train_generator(batch_size)\n",
    "test_generator = create_test_generator(batch_size)\n",
    "resume = False\n",
    "\n",
    "#From start\n",
    "if training_type == TrainingType.SCRATCH:\n",
    "    model, prediction_model = create_models(num_classes=train_generator.num_classes(), weights=None)\n",
    "#From imagenet\n",
    "elif training_type == TrainingType.IMGNET:\n",
    "    model, prediction_model = create_models(num_classes=train_generator.num_classes(), weights=\"imagenet\")\n",
    "#From earlier \n",
    "elif training_type == TrainingType.RESUME:\n",
    "    #snapshot = os.path.join(SNAPSHOTS_DIR, \"custom_resnet50_10.h5\")\n",
    "    snapshot = os.path.join(SNAPSHOTS_DIR, \"custom_resnet50_lolle_06.h5\")\n",
    "    resume = True\n",
    "    model, prediction_model = create_models(num_classes=train_generator.num_classes(), weights=snapshot)\n",
    "\n",
    "callbacks = create_callbacks(model, prediction_model, batch_size, test_generator, train_generator, resume)\n",
    "\n",
    "model.trainable = True\n",
    "\n",
    "# start training\n",
    "model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    #steps_per_epoch = 850,\n",
    "    steps_per_epoch = math.floor(train_generator.size()/batch_size),\n",
    "    epochs = 75,\n",
    "    verbose = 1,\n",
    "    #workers=6,\n",
    "    #validation_data=test_generator,\n",
    "    #validation_steps=math.floor(test_generator.size()/batch_size),\n",
    "    callbacks = callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from keras_retinanet.models import load_model, backbone, resnet\n",
    "from keras_retinanet.utils.image import read_image_bgr\n",
    "from shared import get_session\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Get weights from conv1 layer\n",
    "weights = np.asarray(model.get_layer('conv1').get_weights())\n",
    "fig, ax = plt.subplots(figsize=(10, 10),nrows=8, ncols=8)\n",
    "for i, ax in enumerate(ax.reshape(-1)):\n",
    "    imgArray = weights[0,:,:,:,i]\n",
    "    # unity-based normalization https://datascience.stackexchange.com/a/5888\n",
    "    imgArray = (imgArray - np.min(imgArray)) / (np.max(imgArray) - np.min(imgArray))\n",
    "    ax.imshow(imgArray)\n",
    "    ax.axis('off')\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
