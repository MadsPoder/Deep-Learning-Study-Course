{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madspoderpetersen/deep-traffic/venv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import cv2\n",
    "import keras.preprocessing.image\n",
    "import keras_retinanet\n",
    "import math\n",
    "from keras_retinanet import models, losses\n",
    "from keras_retinanet.models.retinanet import retinanet_bbox\n",
    "from keras_retinanet.models.resnet import resnet50_retinanet\n",
    "from keras_retinanet.preprocessing.csv_generator import CSVGenerator\n",
    "#from generator import CustomCSVGenerator\n",
    "from keras_retinanet.callbacks import RedirectModel\n",
    "#from keras_retinanet.callbacks.eval import Evaluate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from shared import get_session, TrainingType\n",
    "from callbacks import save_weights_vis, Evaluate\n",
    "from paths import *\n",
    "\n",
    "#Create folders\n",
    "if not os.path.exists(SNAPSHOTS_DIR):\n",
    "    os.makedirs(SNAPSHOTS_DIR)\n",
    "    \n",
    "if not os.path.exists(LOGS_DIR):\n",
    "    os.makedirs(LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(num_classes, weights, freeze_backbone=None):\n",
    "    modifier = freeze_model if freeze_backbone else None\n",
    "    model = models.resnet.resnet_retinanet(num_classes, modifier=modifier)\n",
    "    if weights == \"imagenet\":\n",
    "        backbone = models.backbone('resnet50')\n",
    "        imgnet_weights = backbone.download_imagenet()\n",
    "        #Load by_name only inputs into the resnet backbone (Tak Thomas)\n",
    "        model.load_weights(imgnet_weights, skip_mismatch=True, by_name=True)\n",
    "    elif weights:\n",
    "        model.load_weights(weights, skip_mismatch=True)\n",
    "\n",
    "    prediction_model = retinanet_bbox(model=model)\n",
    "    \n",
    "    model.trainable = True\n",
    "\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True \n",
    "\n",
    "    model.compile(\n",
    "        loss={\n",
    "            'regression'    : losses.smooth_l1(),\n",
    "            'classification': losses.focal()\n",
    "        },\n",
    "        optimizer=keras.optimizers.adam(lr=1e-3, clipnorm=0.001),\n",
    "        \n",
    "    )\n",
    "    #metrics=['accuracy'] kan inkluderes i model.compile\n",
    "    \n",
    "    model.trainable = True\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True \n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    return model, prediction_model\n",
    "\n",
    "def create_train_generator(batch_size):\n",
    "    train_generator = CSVGenerator(\n",
    "        TRAIN_GROUND_TRUTH_COMBINED,\n",
    "        CLASS_MAPPINGS_COMBINED,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return train_generator\n",
    "\n",
    "def create_test_generator(batch_size):\n",
    "    test_generator = CSVGenerator(\n",
    "        TEST_GROUND_TRUTH_COMBINED,\n",
    "        CLASS_MAPPINGS_COMBINED,\n",
    "        shuffle_groups=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return test_generator\n",
    "\n",
    "def create_callbacks(model, prediction_model, batch_size, test_generator, train_generator, resume):\n",
    "    callbacks = []\n",
    "\n",
    "    # save the prediction model\n",
    "    if resume:\n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(SNAPSHOTS_DIR, 'custom_resnet50_lolle_resume_imgnet_{epoch:02d}.h5'), verbose=1, period=5)\n",
    "    else:\n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(SNAPSHOTS_DIR, 'custom_resnet50_imgnet_{epoch:02d}.h5'), verbose=1, period=5)\n",
    "\n",
    "    #checkpoint = RedirectModel(checkpoint, model)\n",
    "    callbacks.append(checkpoint)\n",
    "    \n",
    "    #Visualisation of weights\n",
    "    vis_weights = None\n",
    "    if resume:\n",
    "        vis_weights = save_weights_vis(os.path.join(SNAPSHOTS_DIR, 'custom_resnet50_resume_imgnet_{}.png'))\n",
    "    else:\n",
    "        vis_weights = save_weights_vis(os.path.join(SNAPSHOTS_DIR, 'custom_resnet50_imgnet_{}.png'))\n",
    "    callbacks.append(vis_weights)\n",
    "    \n",
    "    #Reduce learning rate on plateu (tidligere loss, val_loss)\n",
    "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, verbose=1, mode='auto', cooldown=0, min_lr=0.001)\n",
    "    #callbacks.append(lr_scheduler)\n",
    "    \n",
    "    #Early stop\n",
    "    early_stop = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "    callbacks.append(early_stop)\n",
    "    \n",
    "    #For tensorboard\n",
    "    tensorboard_callb = keras.callbacks.TensorBoard(log_dir=LOGS_DIR, histogram_freq=0, batch_size=batch_size, \n",
    "                                                    write_graph=True, write_grads=False, write_images=True, \n",
    "                                                    embeddings_freq=0, embeddings_layer_names=None, \n",
    "                                                    embeddings_metadata=None)\n",
    "    callbacks.append(tensorboard_callb)\n",
    "    \n",
    "    #Evaluation for the test/train set\n",
    "    evaluation_test = Evaluate(test_generator, tensorboard=tensorboard_callb, verbose=0, train=False)\n",
    "    evaluation_test = RedirectModel(evaluation_test, prediction_model)\n",
    "    callbacks.append(evaluation_test)\n",
    "    evaluation_train = Evaluate(train_generator, tensorboard=tensorboard_callb, verbose=0, train=True)\n",
    "    evaluation_train = RedirectModel(evaluation_train, prediction_model)\n",
    "    callbacks.append(evaluation_train)\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/madspoderpetersen/deep-traffic/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Epoch 1/75\n",
      "151/151 [==============================] - 147s 972ms/step - loss: 5.1974 - regression_loss: 3.7819 - classification_loss: 1.4155\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_1.png\n",
      "Epoch 2/75\n",
      "151/151 [==============================] - 134s 888ms/step - loss: 3.9670 - regression_loss: 3.1232 - classification_loss: 0.8438\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_2.png\n",
      "Epoch 3/75\n",
      "151/151 [==============================] - 134s 887ms/step - loss: 2.4538 - regression_loss: 1.8714 - classification_loss: 0.5824\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_3.png\n",
      "Epoch 4/75\n",
      "151/151 [==============================] - 134s 888ms/step - loss: 1.8992 - regression_loss: 1.4179 - classification_loss: 0.4813\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_4.png\n",
      "Epoch 5/75\n",
      "151/151 [==============================] - 134s 888ms/step - loss: 1.6276 - regression_loss: 1.1717 - classification_loss: 0.4559\n",
      "\n",
      "Epoch 00005: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_05.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_5.png\n",
      "Epoch 6/75\n",
      "151/151 [==============================] - 135s 891ms/step - loss: 1.4684 - regression_loss: 1.0341 - classification_loss: 0.4344\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_6.png\n",
      "Epoch 7/75\n",
      "151/151 [==============================] - 135s 891ms/step - loss: 1.3191 - regression_loss: 0.9364 - classification_loss: 0.3827\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_7.png\n",
      "Epoch 8/75\n",
      "151/151 [==============================] - 135s 891ms/step - loss: 1.2276 - regression_loss: 0.8869 - classification_loss: 0.3408\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_8.png\n",
      "Epoch 9/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 1.1021 - regression_loss: 0.7777 - classification_loss: 0.3243\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_9.png\n",
      "Epoch 10/75\n",
      "151/151 [==============================] - 135s 892ms/step - loss: 1.0029 - regression_loss: 0.7299 - classification_loss: 0.2730\n",
      "\n",
      "Epoch 00010: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_10.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_10.png\n",
      "Epoch 11/75\n",
      "151/151 [==============================] - 135s 897ms/step - loss: 1.0350 - regression_loss: 0.7666 - classification_loss: 0.2685\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_11.png\n",
      "Epoch 12/75\n",
      "151/151 [==============================] - 135s 897ms/step - loss: 0.9568 - regression_loss: 0.6976 - classification_loss: 0.2592\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_12.png\n",
      "Epoch 13/75\n",
      "151/151 [==============================] - 135s 895ms/step - loss: 0.8473 - regression_loss: 0.6447 - classification_loss: 0.2026\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_13.png\n",
      "Epoch 14/75\n",
      "151/151 [==============================] - 135s 894ms/step - loss: 0.8791 - regression_loss: 0.6258 - classification_loss: 0.2533\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_14.png\n",
      "Epoch 15/75\n",
      "151/151 [==============================] - 135s 897ms/step - loss: 0.8584 - regression_loss: 0.6250 - classification_loss: 0.2333\n",
      "\n",
      "Epoch 00015: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_15.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_15.png\n",
      "Epoch 16/75\n",
      "151/151 [==============================] - 136s 898ms/step - loss: 0.8205 - regression_loss: 0.5946 - classification_loss: 0.2259\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_16.png\n",
      "Epoch 17/75\n",
      "151/151 [==============================] - 136s 899ms/step - loss: 0.7547 - regression_loss: 0.5739 - classification_loss: 0.1808\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_17.png\n",
      "Epoch 18/75\n",
      "151/151 [==============================] - 136s 900ms/step - loss: 0.7735 - regression_loss: 0.5958 - classification_loss: 0.1777\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_18.png\n",
      "Epoch 19/75\n",
      "151/151 [==============================] - 136s 900ms/step - loss: 0.7171 - regression_loss: 0.5446 - classification_loss: 0.1725\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_19.png\n",
      "Epoch 20/75\n",
      "151/151 [==============================] - 136s 899ms/step - loss: 0.7147 - regression_loss: 0.5479 - classification_loss: 0.1668\n",
      "\n",
      "Epoch 00020: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_20.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_20.png\n",
      "Epoch 21/75\n",
      "151/151 [==============================] - 135s 895ms/step - loss: 0.6570 - regression_loss: 0.5054 - classification_loss: 0.1517\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_21.png\n",
      "Epoch 22/75\n",
      "151/151 [==============================] - 135s 895ms/step - loss: 0.6254 - regression_loss: 0.5061 - classification_loss: 0.1194\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_22.png\n",
      "Epoch 23/75\n",
      "151/151 [==============================] - 135s 897ms/step - loss: 0.5664 - regression_loss: 0.4599 - classification_loss: 0.1065\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_23.png\n",
      "Epoch 24/75\n",
      "151/151 [==============================] - 136s 902ms/step - loss: 0.5749 - regression_loss: 0.4542 - classification_loss: 0.1207\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_24.png\n",
      "Epoch 25/75\n",
      "151/151 [==============================] - 136s 901ms/step - loss: 0.5664 - regression_loss: 0.4558 - classification_loss: 0.1106\n",
      "\n",
      "Epoch 00025: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_25.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_25.png\n",
      "Epoch 26/75\n",
      "151/151 [==============================] - 136s 899ms/step - loss: 0.5296 - regression_loss: 0.4468 - classification_loss: 0.0828\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_26.png\n",
      "Epoch 27/75\n",
      "151/151 [==============================] - 136s 900ms/step - loss: 0.5089 - regression_loss: 0.4331 - classification_loss: 0.0758\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_27.png\n",
      "Epoch 28/75\n",
      "151/151 [==============================] - 136s 900ms/step - loss: 0.4841 - regression_loss: 0.4038 - classification_loss: 0.0803\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_28.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/75\n",
      "151/151 [==============================] - 136s 901ms/step - loss: 0.5202 - regression_loss: 0.4286 - classification_loss: 0.0916\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_29.png\n",
      "Epoch 30/75\n",
      "151/151 [==============================] - 136s 898ms/step - loss: 0.4454 - regression_loss: 0.3811 - classification_loss: 0.0643\n",
      "\n",
      "Epoch 00030: saving model to /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_30.h5\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_30.png\n",
      "Epoch 31/75\n",
      "151/151 [==============================] - 135s 891ms/step - loss: 0.4618 - regression_loss: 0.3954 - classification_loss: 0.0663\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_31.png\n",
      "Epoch 32/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 0.4475 - regression_loss: 0.3811 - classification_loss: 0.0664\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_32.png\n",
      "Epoch 33/75\n",
      "151/151 [==============================] - 135s 893ms/step - loss: 0.4179 - regression_loss: 0.3694 - classification_loss: 0.0485\n",
      "Saving visualised weights of conv1 to file: /home/madspoderpetersen/deep-traffic/snapshots/custom_resnet50_imgnet_33.png\n",
      "Epoch 34/75\n",
      " 42/151 [=======>......................] - ETA: 1:37 - loss: 0.4432 - regression_loss: 0.3935 - classification_loss: 0.0496"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-91a93c453c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#validation_data=test_generator,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#validation_steps=math.floor(test_generator.size()/batch_size),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m )\n",
      "\u001b[0;32m~/deep-traffic/venv/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-traffic/venv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-traffic/venv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-traffic/venv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-traffic/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-traffic/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-traffic/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-traffic/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-traffic/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-traffic/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = None\n",
    "training_type = TrainingType.IMGNET\n",
    "batch_size = 4\n",
    "\n",
    "keras.backend.tensorflow_backend.set_session(get_session())\n",
    "train_generator = create_train_generator(batch_size)\n",
    "test_generator = create_test_generator(batch_size)\n",
    "resume = False\n",
    "\n",
    "#From start\n",
    "if training_type == TrainingType.SCRATCH:\n",
    "    model, prediction_model = create_models(num_classes=train_generator.num_classes(), weights=None)\n",
    "#From imagenet\n",
    "elif training_type == TrainingType.IMGNET:\n",
    "    model, prediction_model = create_models(num_classes=train_generator.num_classes(), weights=\"imagenet\")\n",
    "#From earlier \n",
    "elif training_type == TrainingType.RESUME:\n",
    "    #snapshot = os.path.join(SNAPSHOTS_DIR, \"custom_resnet50_10.h5\")\n",
    "    snapshot = os.path.join(SNAPSHOTS_DIR, \"custom_resnet50_lolle_06.h5\")\n",
    "    resume = True\n",
    "    model, prediction_model = create_models(num_classes=train_generator.num_classes(), weights=snapshot)\n",
    "\n",
    "callbacks = create_callbacks(model, prediction_model, batch_size, test_generator, train_generator, resume)\n",
    "\n",
    "model.trainable = True\n",
    "\n",
    "# start training\n",
    "model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    #steps_per_epoch = 850,\n",
    "    steps_per_epoch = math.floor(train_generator.size()/batch_size),\n",
    "    epochs = 75,\n",
    "    verbose = 1,\n",
    "    #workers=6,\n",
    "    #validation_data=test_generator,\n",
    "    #validation_steps=math.floor(test_generator.size()/batch_size),\n",
    "    callbacks = callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from keras_retinanet.models import load_model, backbone, resnet\n",
    "from keras_retinanet.utils.image import read_image_bgr\n",
    "from shared import get_session\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Get weights from conv1 layer\n",
    "weights = np.asarray(model.get_layer('conv1').get_weights())\n",
    "fig, ax = plt.subplots(figsize=(10, 10),nrows=8, ncols=8)\n",
    "for i, ax in enumerate(ax.reshape(-1)):\n",
    "    imgArray = weights[0,:,:,:,i]\n",
    "    # unity-based normalization https://datascience.stackexchange.com/a/5888\n",
    "    imgArray = (imgArray - np.min(imgArray)) / (np.max(imgArray) - np.min(imgArray))\n",
    "    ax.imshow(imgArray)\n",
    "    ax.axis('off')\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
